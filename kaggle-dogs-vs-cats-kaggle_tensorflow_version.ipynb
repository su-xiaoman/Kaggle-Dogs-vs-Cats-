{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T02:53:40.186520Z",
     "iopub.status.busy": "2022-05-13T02:53:40.186235Z",
     "iopub.status.idle": "2022-05-13T02:53:40.211073Z",
     "shell.execute_reply": "2022-05-13T02:53:40.209682Z",
     "shell.execute_reply.started": "2022-05-13T02:53:40.186489Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "\n",
    "import h5py\n",
    "import glob\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.random.set_seed(22)\n",
    "np.random.seed(2022)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件解压，数据显示，以及本文所使用的参考链接\n",
    "- 多模型特征向量组合方式:[参考1](https://zhuanlan.zhihu.com/p/25978105)\n",
    "- 特征提取介绍: [参考2](https://dev.mrdbourke.com/tensorflow-deep-learning/04_transfer_learning_in_tensorflow_part_1_feature_extraction/)\n",
    "- Tensorflow迁移学习介绍: [参考3](https://tensorflow.google.cn/tutorials/images/transfer_learning?hl=en)\n",
    "- 用于微调的checkpoint: [参考4](https://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models)\n",
    "- Tensorflow hub特征向量与模型汇总: [参考5](https://hub.tensorflow.google.cn/s?module-type=image-feature-vector&tf-version=tf2)\n",
    "- 最终kaggle提交结果: [最佳结果:0.03694](./best_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T02:48:12.457315Z",
     "iopub.status.busy": "2022-05-13T02:48:12.456968Z",
     "iopub.status.idle": "2022-05-13T02:48:25.783804Z",
     "shell.execute_reply": "2022-05-13T02:48:25.782832Z",
     "shell.execute_reply.started": "2022-05-13T02:48:12.457276Z"
    }
   },
   "outputs": [],
   "source": [
    "# 解压部分 将数据集文件夹拷贝到当前工作目录下面来,如果没有解压，可以使用下述代码自动解压，\n",
    "# ../input/dogs-vs-cats-redux-kernels- 是一个只读文件夹，只能解压到当前working\n",
    "def dataset_extractor(base_dir):\n",
    "    train_dir = os.path.join(base_dir, \"train.zip\")\n",
    "    test_dir = os.path.join(base_dir, \"test.zip\")\n",
    "    if not os.path.exists(\"./dogs-vs-cats-redux-kernels-edition/train\"):#当前目录没有此文件，说明没有解压\n",
    "        with zipfile.ZipFile(train_dir,\"r\") as z:\n",
    "            z.extractall(\"dogs-vs-cats-redux-kernels-edition\")\n",
    "            z.close()\n",
    "    if not os.path.exists(\"./dogs-vs-cats-redux-kernels-edition/test\"):#当前目录没有此文件，说明没有解压\n",
    "        with zipfile.ZipFile(test_dir,\"r\") as z:\n",
    "            z.extractall(\"dogs-vs-cats-redux-kernels-edition\")\n",
    "            z.close()\n",
    "            \n",
    "base_dir = \"../input/dogs-vs-cats-redux-kernels-edition\"         \n",
    "dataset_extractor(base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T02:49:36.770118Z",
     "iopub.status.busy": "2022-05-13T02:49:36.769831Z",
     "iopub.status.idle": "2022-05-13T02:49:42.875352Z",
     "shell.execute_reply": "2022-05-13T02:49:42.874350Z",
     "shell.execute_reply.started": "2022-05-13T02:49:36.770085Z"
    }
   },
   "outputs": [],
   "source": [
    "#数据处理，将原文件夹下混合的猫狗文件拷贝到当前目录下来并且分类\n",
    "def create_dataset():\n",
    "    train_cat = glob.glob('./dogs-vs-cats-redux-kernels-edition/train/cat*.jpg')\n",
    "    train_dog = glob.glob('./dogs-vs-cats-redux-kernels-edition/train/dog*.jpg')\n",
    "    test_dog_cat = glob.glob('./dogs-vs-cats-redux-kernels-edition/test/*')\n",
    "\n",
    "    if not os.path.exists('train'):\n",
    "        os.makedirs('train/cat',exist_ok=True)\n",
    "        os.makedirs('train/dog',exist_ok=True)\n",
    "        for filename in train_cat:\n",
    "            shutil.copy(filename, 'train/cat/'+os.path.basename(filename))\n",
    "        for filename in train_dog:\n",
    "            shutil.copy(filename, 'train/dog/'+os.path.basename(filename))\n",
    "    if not os.path.exists('test'):\n",
    "        os.makedirs('test/cat_dog',exist_ok=True)\n",
    "        for filename in test_dog_cat:\n",
    "            shutil.copy(filename,'test/cat_dog/'+os.path.basename(filename))\n",
    "            \n",
    "create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T02:49:54.321138Z",
     "iopub.status.busy": "2022-05-13T02:49:54.320718Z",
     "iopub.status.idle": "2022-05-13T02:49:56.640059Z",
     "shell.execute_reply": "2022-05-13T02:49:56.639195Z",
     "shell.execute_reply.started": "2022-05-13T02:49:54.321092Z"
    }
   },
   "outputs": [],
   "source": [
    "#图像展示 PIL.Image + plt.imshow\n",
    "def show_image(file_path):\n",
    "    file_list = glob.glob(file_path)\n",
    "    fig = plt.figure(figsize=(12,16))\n",
    "    for i in range(5):\n",
    "        img = Image.open(file_list[i])\n",
    "        sub_img = fig.add_subplot(151+i)\n",
    "        sub_img.imshow(img)\n",
    "\n",
    "train_cats_list = \"./train/cat/*\"\n",
    "train_dogs_list = \"./train/dog/*\"\n",
    "test_dogs_list = \"./test/cat_dog/*\"\n",
    "#可以看到图像的大小填充程度均不统一，因此需要进行进一步的处理    \n",
    "# show_image(real_cats)\n",
    "show_image(train_cats_list)\n",
    "show_image(train_dogs_list)\n",
    "show_image(test_dogs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征向量的提取\n",
    "- 此版本为kaggle版本\n",
    "- 由于原始版本的迁移学习在训练一个epoch时极其缓慢，在服务器Tesla P40-24GB上也需要大概3分钟左右生成一次epoch，这对于调节超参数极其煎熬，所以选择先导出特征向量然后作为新的感知机模型输入，这几乎可能在30s之内完成对于10个epoch的训练\n",
    "- 本模型测试了：EfficientNetV2B0/ResNet50/ResNet101/ResNet152/Xception/InceptionV3/VGG16/VGG19多种模型在单一和组合条件下的效果\n",
    "- 包括在没有数据增强的条件下/使用不包括将\\[0,255\\]放缩到\\[0,1\\]/使用包括放缩在内的全部常见的数据增强(后证明不应该加入此选项，应使用keras.application选项中给予的自动调节preprocess_input)\n",
    "- 最优结果出现在使用：1. 没有数据增强的'Xception.h5','InceptionV3.h5' 2. 使用了数据增强的'EfficientNetV2B0_plus.h5' 共三个模型的组合选项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:14:42.999073Z",
     "iopub.status.busy": "2022-05-13T04:14:42.998376Z",
     "iopub.status.idle": "2022-05-13T04:14:43.018470Z",
     "shell.execute_reply": "2022-05-13T04:14:43.016981Z",
     "shell.execute_reply.started": "2022-05-13T04:14:42.999040Z"
    }
   },
   "outputs": [],
   "source": [
    "# transfer learning\n",
    "def use_pretrained_model_parameter(MODEL, image_size, lambda_func=None):\n",
    "    #定义不同的模型所需要的大小\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    print(width,height) #224*224 #229*229\n",
    "    \n",
    "    #tf.keres.utils.image_dataset_from_directory直接生成tf.data.Dataset对于，我们需要手动去提取其标签，\n",
    "    #但是tf.keras.utils.image_dataset_from_directory强制切片，使得我们需要将标签重新拼接起来\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\"train\",\n",
    "                                                               image_size=image_size,\n",
    "                                                               batch_size=256,\n",
    "                                                               shuffle=False,\n",
    "                                                                labels='inferred',\n",
    "                                                                label_mode=\"int\"\n",
    "                                                               )\n",
    "\n",
    "    test_dataset = tf.keras.utils.image_dataset_from_directory(\"test\",\n",
    "                                                               image_size=image_size,\n",
    "                                                               batch_size=256,\n",
    "                                                               shuffle=False,\n",
    "                                                                labels='inferred',\n",
    "                                                               label_mode=\"int\"\n",
    "                                                               )\n",
    "        \n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Resizing(256,256),#应该能够被image_size替代，但是不知道为什么不行。\n",
    "        tf.keras.layers.CenterCrop(height,width), #借助中心裁剪成输入所需要的大小，替代resize\n",
    "        tf.keras.layers.RandomFlip('horizontal'),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.5),        \n",
    "    ])\n",
    "    \n",
    "#     #定义模型(这里仅仅为训练集定义了数据增强，已经测试版本同时使用了数据增强，下列代码未经测试，如果跑不通，可以合并train_x/test_x=>x)   \n",
    "    train_x = input_tensor\n",
    "    test_x = input_tensor\n",
    "    \n",
    "    train_x = data_augmentation(train_x)  \n",
    "    test_x = test_x #rescale和resize已经默认做的，因此训练样本不经过任何处理\n",
    "    #由于预处理范围不一定一样，因此使用模型默认的preprocess_input去替代数据增强的rescale\n",
    "    if lambda_func:\n",
    "        train_x = Lambda(lambda_func)(train_x)\n",
    "        test_x = Lambda(lambda_func)(test_x)\n",
    "    train_base_model = MODEL(input_tensor=train_x, weights='imagenet', include_top=False)\n",
    "    test_base_model = MODEL(input_tensor=test_x, weights='imagenet', include_top=False)\n",
    "    #print(base_model.input.shape)\n",
    "#     print(base_model.output.shape)\n",
    "    model1 = Model(train_base_model.input,\n",
    "                  GlobalAveragePooling2D()(train_base_model.output))\n",
    "    model2 = Model(test_base_model.input,\n",
    "                  GlobalAveragePooling2D()(test_base_model.output))\n",
    "\n",
    "    \n",
    "    #这里的train包含着对于train_dataset的预测结果，其可能包含着对于x_data,y_label\n",
    "    train = model1.predict(train_dataset)\n",
    "    test = model2.predict(test_dataset)\n",
    "    \n",
    "    \n",
    "    y_label = np.concatenate([y for x, y in train_dataset], axis=0)\n",
    "    print(\"train_shape\",train.shape) #(25000, 1280) (12500, 1280)\n",
    "    print(\"test_shape\",test.shape)\n",
    "    print(y_label.shape)\n",
    "          \n",
    "        \n",
    "    with h5py.File(\"%s.h5\"%MODEL.__name__,'w') as f:\n",
    "        f.create_dataset(\"train\", data=train)\n",
    "        f.create_dataset(\"test\", data=test)\n",
    "        f.create_dataset(\"label\", data=y_label)\n",
    "        print(\"success\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:14:43.952263Z",
     "iopub.status.busy": "2022-05-13T04:14:43.951767Z",
     "iopub.status.idle": "2022-05-13T04:27:58.182487Z",
     "shell.execute_reply": "2022-05-13T04:27:58.181477Z",
     "shell.execute_reply.started": "2022-05-13T04:14:43.952200Z"
    }
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists('*.h5'): #可选的下载\n",
    "# from tensorflow.keras.applications import EfficientNetV2B0\n",
    "use_pretrained_model_parameter(EfficientNetB0,(224,224),efficientnet.preprocess_input)\n",
    "use_pretrained_model_parameter(Xception, (299, 299), xception.preprocess_input)\n",
    "use_pretrained_model_parameter(InceptionV3, (299, 299), inception_v3.preprocess_input)\n",
    "use_pretrained_model_parameter(InceptionResNetV2,(299, 299),inception_resnet_v2.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:31:21.422053Z",
     "iopub.status.busy": "2022-05-13T04:31:21.421435Z",
     "iopub.status.idle": "2022-05-13T04:31:23.467248Z",
     "shell.execute_reply": "2022-05-13T04:31:23.465839Z",
     "shell.execute_reply.started": "2022-05-13T04:31:21.422020Z"
    }
   },
   "outputs": [],
   "source": [
    "#准备数据\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "pretrain_model = ['InceptionResNetV2.h5','EfficientNetB0.h5','InceptionV3.h5','Xception.h5',]#'ResNet50.h5',\n",
    "for filename in pretrain_model: # \n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        y_train = np.array(h['label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape)\n",
    "# print(pretrain_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型\n",
    "- 测试结果表明：\n",
    "- dropout率设置为0.45/0.5两个值会拥有更好的效果\n",
    "- 增加更多的全连接层会显著降低kaggle得分\n",
    "- Adam设置为1e-3时，kaggle得分会更稳定保持0.037XX,经过多次试验，其最好成绩为0.036XX\n",
    "- 对于model.fit=>epochs设置为9或者10,低于此值或高于此值均降低kaggle得分，validation_split选项应去掉，即在生成提交数据时完全不使用验证集\n",
    "- 由于kaggle的评估计算是基于logloss的，其对于错误差距的小数点极其敏感，因此应该参考预测结果对预测进行相应裁剪，如本模型中为0.9956xxx,将y_pred.clip裁剪到\\[0.003,0.995\\]，不加入裁剪会降低到0.038左右 \n",
    "- checkpoint和model两种预训练均已给出，可以反注释掉相应的地方以使用\n",
    "- 无论是使用batchnorm还是layernorm都不能增加得分\n",
    "- 加入正则化基本不会起任何作用kernel_regularizer=keras.regularizers.l2(1e-4)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:32:03.041498Z",
     "iopub.status.busy": "2022-05-13T04:32:03.041199Z",
     "iopub.status.idle": "2022-05-13T04:32:03.319401Z",
     "shell.execute_reply": "2022-05-13T04:32:03.318365Z",
     "shell.execute_reply.started": "2022-05-13T04:32:03.041466Z"
    }
   },
   "outputs": [],
   "source": [
    "#引入sklearn计算log_loss,引入Keras.wrapper来显式地支持sklearn格式，如果不需要，可以自行换成标准版本\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "print(X_train.shape,X_val.shape)\n",
    "print(y_train.shape,y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:32:14.568003Z",
     "iopub.status.busy": "2022-05-13T04:32:14.567694Z",
     "iopub.status.idle": "2022-05-13T04:32:14.807930Z",
     "shell.execute_reply": "2022-05-13T04:32:14.806878Z",
     "shell.execute_reply.started": "2022-05-13T04:32:14.567970Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "def create_model():\n",
    "#     X_train,y_train = shuffle(train_EB7_ns,target)\n",
    "\n",
    "    #可选:使用已经训练的模型\n",
    "    # if os.path.exists('model-kaggle-dogs-vs-cats'):\n",
    "    #     tf.keras.models.load_model('./model-kaggle-dogs-vs-cats')\n",
    "    # else:\n",
    "    input_tensor = Input(X_train.shape[1:])\n",
    "    x = input_tensor\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    # x = Dense(1024,activation='relu',kernel_regularizer=keras.regularizers.l2(1e-3))(x) #加上一层全连接并不能加大准确率\n",
    "    # x = Dropout(0.2)(x)\n",
    "    x = Dense(1,activation='sigmoid')(x) \n",
    "\n",
    "\n",
    "    #in Model(first_layer,last_layer)    \n",
    "    model = Model(input_tensor, x)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    #               metrics=['binary_accuracy'],\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:32:30.535108Z",
     "iopub.status.busy": "2022-05-13T04:32:30.534814Z",
     "iopub.status.idle": "2022-05-13T04:32:37.391818Z",
     "shell.execute_reply": "2022-05-13T04:32:37.390859Z",
     "shell.execute_reply.started": "2022-05-13T04:32:30.535078Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "clf1 = KerasClassifier(build_fn=create_model, verbose=1, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# eclf1 = BaggingClassifier(clf1,n_estimators=5)\n",
    "clf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:33:06.797555Z",
     "iopub.status.busy": "2022-05-13T04:33:06.797236Z",
     "iopub.status.idle": "2022-05-13T04:33:06.802880Z",
     "shell.execute_reply": "2022-05-13T04:33:06.801590Z",
     "shell.execute_reply.started": "2022-05-13T04:33:06.797522Z"
    }
   },
   "outputs": [],
   "source": [
    "# #绘制与可视化模型 仅仅对于非sklearn-KerasClassifier可以使用\n",
    "# tf.keras.utils.plot_model(clf1,show_shapes=True,show_layer_names=True)\n",
    "# #查看每层和参数\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T10:46:34.867357Z",
     "iopub.status.busy": "2022-04-30T10:46:34.867088Z",
     "iopub.status.idle": "2022-04-30T10:46:46.317827Z",
     "shell.execute_reply": "2022-04-30T10:46:46.316955Z",
     "shell.execute_reply.started": "2022-04-30T10:46:34.867326Z"
    }
   },
   "outputs": [],
   "source": [
    "#标准训练版本\n",
    "#随着训练量的增加，在训练集上和验证集上均开始有着非常好的结果。但是kaggle上的提交得到再减少，应该是明显地过拟合，建议适当屏蔽掉checkpoint\n",
    "# if os.path.exists('./checkpoint/'):\n",
    "#     lastest = tf.train.latest_checkpoint('./checkpoint')\n",
    "#     print('load latest checkpoint!!')\n",
    "#     model.load_weights(lastest)\n",
    "# # # all_cp = tf.train.list_variables('./checkpoint')\n",
    "# # # print(all_cp)\n",
    "# checkpoint_path = \"./checkpoint/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "# batch_size = 256\n",
    "# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                   save_weights_only=True,\n",
    "#                                                   save_best_only=True,\n",
    "#                                                    verbose=1,\n",
    "#                                                    save_freq=batch_size\n",
    "#                                                   )\n",
    "# import datetime\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# print(\"start:\",X_train.shape,y_train.shape)\n",
    "# #EB7_X_train, EB7_X_val, EB7_y_train, EB7_y_val\n",
    "\n",
    "# X_train,X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "# print(X_train.shape,y_train.shape)\n",
    "# print(X_val.shape,y_val.shape)\n",
    "# combined_model_history = model.fit(X_train, y_train, \n",
    "#                                    batch_size=batch_size, \n",
    "#                                    epochs=6,\n",
    "#                                    validation_split=0.03,\n",
    "#                                    callbacks=[ckpt_callback,tensorboard_callback]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:33:29.898050Z",
     "iopub.status.busy": "2022-05-13T04:33:29.897737Z",
     "iopub.status.idle": "2022-05-13T04:33:30.256569Z",
     "shell.execute_reply": "2022-05-13T04:33:30.255533Z",
     "shell.execute_reply.started": "2022-05-13T04:33:29.898018Z"
    }
   },
   "outputs": [],
   "source": [
    "#sklearn既有软标签，也有硬标签，而keras只有软标签\n",
    "# print(model.predict(X_val).shape)\n",
    "nn_pred = clf1.predict_proba(X_val)[:,1]\n",
    "\n",
    "print(roc_auc_score(y_val, nn_pred))\n",
    "print(log_loss(y_val, nn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:33:45.634421Z",
     "iopub.status.busy": "2022-05-13T04:33:45.634098Z",
     "iopub.status.idle": "2022-05-13T04:33:45.648135Z",
     "shell.execute_reply": "2022-05-13T04:33:45.647077Z",
     "shell.execute_reply.started": "2022-05-13T04:33:45.634388Z"
    }
   },
   "outputs": [],
   "source": [
    "#绘制损失曲线\n",
    "# def plot_loss_curves(history):\n",
    "#     acc = history.history['accuracy']\n",
    "#     loss = history.history['loss']\n",
    "\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "#     val_loss = history.history['val_loss']\n",
    "\n",
    "#     epochs = range(len(acc))\n",
    "#     with plt.xkcd():\n",
    "        \n",
    "#         plt.figure(figsize=(20,6))\n",
    "        \n",
    "#         plt.subplot(1,2,1)\n",
    "#         plt.plot(epochs,acc,'b',label='Training accuracy')\n",
    "#         plt.plot(epochs,val_acc,'r',label='Validation accuracy')\n",
    "#         plt.title('accuracy!')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.subplot(1,2,2)\n",
    "#         plt.plot(epochs,loss,'b',label='Training loss')\n",
    "#         plt.plot(epochs,val_loss,'r',label='Validation loss')\n",
    "#         plt.title('Loss!')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.legend()\n",
    "        \n",
    "#         plt.show()\n",
    "\n",
    "# plot_loss_curves(combined_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:33:46.676603Z",
     "iopub.status.busy": "2022-05-13T04:33:46.675979Z",
     "iopub.status.idle": "2022-05-13T04:33:46.681244Z",
     "shell.execute_reply": "2022-05-13T04:33:46.679892Z",
     "shell.execute_reply.started": "2022-05-13T04:33:46.676568Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.save('model-kaggle-dogs-vs-cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:40:51.861157Z",
     "iopub.status.busy": "2022-05-13T04:40:51.858676Z",
     "iopub.status.idle": "2022-05-13T04:40:53.010237Z",
     "shell.execute_reply": "2022-05-13T04:40:53.008995Z",
     "shell.execute_reply.started": "2022-05-13T04:40:51.861123Z"
    }
   },
   "outputs": [],
   "source": [
    "#模型预测\n",
    "y_pred = clf1.predict_proba(X_test)[:,1] #这里的测试集实际上就来自于X_test.append(np.array(h['test']))的测试集\n",
    "# y_pred[y_pred > 0.9973] = 1\n",
    "# y_pred[y_pred <  0.0018] = 0\n",
    "# y_pred = y_pred.clip(min=0.003, max=0.995)\n",
    "y_pred = y_pred.clip(min=0.0018,max=0.9973)\n",
    "print(y_pred.max(),y_pred.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T04:40:58.312097Z",
     "iopub.status.busy": "2022-05-13T04:40:58.311519Z",
     "iopub.status.idle": "2022-05-13T04:40:58.989047Z",
     "shell.execute_reply": "2022-05-13T04:40:58.986984Z",
     "shell.execute_reply.started": "2022-05-13T04:40:58.312065Z"
    }
   },
   "outputs": [],
   "source": [
    "#将预测结果导出到csv\n",
    "# kaggle-cats-vs-dogs-machinelearning kaggle-cats-vs-dogs-machinelearning\n",
    "df= pd.read_csv('../input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv')\n",
    "\n",
    "test_gen = ImageDataGenerator()\n",
    "test_generator = test_gen.flow_from_directory(\"test\",shuffle=False)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    #按照文件名取出索引，如cat_dog/1.jpg=>1,cat_dog/100.jpg=>100\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    #事实上，由于test_generator.filenames所产生的预测是y_pred[1]=>1.jpg,y_pred[2]=>100.jpg，而csv_id按照顺序来\n",
    "    #所以，只能使用从文件名中提取出来的数，然后使用df.at去定位，但是这种定位按照列数定位，由于行首标签占用一行，因此需要减去\n",
    "    df.at[index-1, 'label'] =  y_pred[i]\n",
    "\n",
    "df.to_csv('sub_prediction3.csv', index=None)\n",
    "print(\"print success!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
